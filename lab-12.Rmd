---
title: "Lab 12 - Colonizing Mars"
author: "Eric Stone"
date: "6.4.24"
output: github_document
---

### Load packages and data

```{r load-packages, message=FALSE}
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)
if (!require("MASS")) install.packages("MASS")
library(MASS)
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
```

I'm not certain why we're doing it this way, rather than just loading the packages. Let's discuss this when we meet.

### Exercise 1.1

> 1.Create a dataframe to store your colonists’ attributes. I’ve already gotten you started, but you’ll need to add their ages.

```{r set-seed, message=FALSE}
set.seed(123)
```

```{r create-data-frame, message=FALSE}
df_colonists <- data.frame(id = 1:100, age = rnorm(100, mean = 30, sd = 5))
```

> Now, let’s visualize the age distribution of our colonists using a histogram. This will help us understand the diversity within our potential Mars colony.

```{r creating-histogram, message=FALSE}
ggplot(data = df_colonists, mapping = aes(x = age)) +
  geom_histogram() +
  labs(title = "Distribution of Colonist Ages")
```

Most of the ages are between 20 and 40, as would be expected by the mean and sd's, but there are some over 40 and under 20.


### Exercise 1.2

> Consider the histograms of the age distribution that we’ve generated (as well as two more I added). What do you notice about the age distribution of our colonists? How does the seed affect the spread of the distribution?

I think the idea here is to see how the shape looks with different seeds.  So I redid this, with different seeds. 

```{r create-data-frame-100, message=FALSE}
set.seed(100)
df_colonists_seed100 <- data.frame(id = 1:100, age = rnorm(100, mean = 30, sd = 5))
ggplot(data = df_colonists_seed100, mapping = aes(x = age)) +
  geom_histogram() +
  labs(title = "Distribution of Colonist Ages, Seed 100")
```

```{r create-data-frame-1000, message=FALSE}
set.seed(1000)
df_colonists_seed1000 <- data.frame(id = 1:100, age = rnorm(100, mean = 30, sd = 5))
ggplot(data = df_colonists_seed1000, mapping = aes(x = age)) +
  geom_histogram() +
  labs(title = "Distribution of Colonist Ages, Seed 1000")
```

The distributions are slightly different, as would be expected. 

Note in what you (Mason) showed, they were on the same graph. I'm not seeing how you did that. That would be a good thing to discuss too.

* create 3 vectors, and then combine them into a dataframe with 
* df <- data.frame (age, age2, age3)

The above two lines were from Mason, so I'm going to try this out.  


```{r create-data-frame-combined, message=FALSE}
set.seed(123)
age_vector_123 <- rnorm(100, mean = 30, sd = 5)
set.seed(100)
age_vector_100 <- rnorm(100, mean = 30, sd = 5)
set.seed(1000)
age_vector_1000 <- rnorm(100, mean = 30, sd = 5)
age_df <- data.frame (age_vector_123, age_vector_100, age_vector_1000)
```

Yea. This worked!

> What roles would these colonists have? Let’s decide….. We need engineers, scientists, and medics in equal numbers. We’ll create a variable, role, with three categories: engineer, scientist, and medic. We’ll use the rep() function to simulate this categorical variable. I’ve demonstrated several ways you can use rep() to create the role variable. Try them out.

```{r new-data-frame-with-roles, message=FALSE}
set.seed(1)
df_colonists_roles <- data.frame(id = 1:100, age = rnorm(100, mean = 30, sd = 5), role1 = rep(c("engineer", "scientist", "medic"), length.out = 100), role2 = rep(c("engineer", "scientist", "medic"), each = 34, length.out = 100), role3 = rep(c("engineer", "scientist", "medic"), times = c(33, 33, 33), length.out = 100), role4 = sample(c("engineer", "scientist", "medic"), replace = TRUE, size = 100, prob = c(1,1,1)) ) 
df_colonists_roles %>%
  pivot_longer(cols = starts_with("role"), names_to = "role_type", values_to = "role") %>%
  count(role_type, role)
```

(yes, I got some help from chat for the pivot_longer command, which I don't really understand)

At any rate, here's what I can surmise:

* Method 1 and Method 3 seem to be the same. (I tried this with multiple seeds, and it always produced the exact same results.) In both cases, ID 1 was an engineer, ID 2 a scientist, ID 3 a medic, ID 4 an engineer, etc.
* Method 2 gave 34 engineers and scientists (the first two professions listed), and then tried to get 34 medics, but there were only 32 people left so there are only 32 medics.
* Method 4 appears to choose probabilistically (e.g., ID 1 has a 1/3 chance of being an engineer, scientist, or medic; same with ID 2, etc). This in one sense is the best approach, but likely in practice we'll have substantial deviation from equality just by chance, as happened here.

> 1.3. Create a role variable that suites the needs of your colony, and explain why you chose that method.

Note to Mason:  'suits' rather than 'suites' :)

Honestly, I don't really like any of the methods.  Method 4 produces unequal n's.  Method 1&3 are the closest to unequal n's, but I have two problems with this approach:

1) Which profession has 34 isn't random

2) More importantly, which ID's get which professions isn't random. Because age was random, I don't think that's a problem here. But let's say that we assigned age categorically, via age = rep(c("young", "mature", "old"), length.out = 100).  Now, EVERY engineer is young, every medic is old, etc. This seems like a major problem.  

This issue could perhaps be solved with block randomization rather than using the same order for each block of 3, but what I'd like would be to just randomly assign 33 of the IDs to engineer, 33 to scientist, and 33 to medic. This has to be doable, right???

Given the options, I think I have to go with Method 4, but I'm not crazy about it because I wanted more of a uniform (identical) distribution of each of the roles.


```{r resetting-seed, message=FALSE}
set.seed(123)
```

```{r adding_role_to-data_frame, message=FALSE}
df_colonists <- df_colonists %>% mutate (role = sample(c("engineer", "scientist", "medic"), replace = TRUE, size = 100, prob = c(1,1,1)) )
```

> 1.4. Add a uniformally-distributed MARSGAR variable to your colony.

```{r adding_marsgar_to_data_frame, message=FALSE}
df_colonists$marsgar <- runif(100,min = 50, max = 100)
```

This seems to have added marsgar to my data frame!!!!!! Yea. I thought I had to use mutate every time -- this is much easier.

Note I started my range at 50 rather than 0 so that my colonists aren't dead .....

```{r conducting_scatterplot, message=FALSE}
ggplot(data = df_colonists, mapping = aes(x = age, y = marsgar)) +
  geom_point() +
  labs(title = "Relationship between age and marsgar")
```



### Exercise 2

> Simulate technical skills based on age using the equation technical_skills = 2 * age + noise.

```{r changing_seed, message=FALSE}
set.seed(1235)
```

We're changing the seed that we used to create the other variables in the dataset (from 123 to 1235). I think there's no particular reason to do this, but please let me know if that's wrong!

```{r creating_technical_skills, message=FALSE}
df_colonists$technical_skills <- 2 * df_colonists$age + rnorm(100, mean = 0, sd = 1)
```

```{r another_scatterplot, message=FALSE}
ggplot(data = df_colonists, mapping = aes(x = age, y = technical_skills)) +
  geom_point() +
  labs(title = "Relationship between age and technical_skills")
```

Hmm, I wish technical skills went up that clearly with age, but as the Mason and Eric data points indicate, this is an unrealistic graph....

> 2.2. Simulate problem-solving abilities based on their assigned role.
Now its your turn! Simulate problem-solving abilities based on their assigned role. Recall that the options are “engineer”, “scientist”, “medic”. First you need to think about the relationship between the role AND the variable being simulated. Then you can write up the equation that would create that relationship. And then you can write the code to simulate the variable.

I'm going to assume problem solving ranges from 0 to 100.  I'm going to make the following other assumptions:

1) The rank ordering of problem solving ability is (from best to worst): engineer, scientist, medic
2) However, the three professions are very similar in problem solving ability, and there is substantial overlap among the distributions
3) In general, the problem solving ability is quite high

I'll make the means 72, 74, and 76, and with a constant sd of 6. In actuality, the sd should probably be larger, but I don't want to get a score greater than 100 and i'm not sure how to deal with that possibility. There also should probably be some negative skew, but this should suffice for now.

```{r adding_problem_solving, message=FALSE}
df_colonists$problem_solving[df_colonists$role == "engineer"] <- rnorm(sum(df_colonists$role == "engineer"), mean = 76, sd = 6)
df_colonists$problem_solving[df_colonists$role == "scientist"] <- rnorm(sum(df_colonists$role == "scientist"), mean = 74, sd = 6)
df_colonists$problem_solving[df_colonists$role == "medic"] <- rnorm(sum(df_colonists$role == "medic"), mean = 72, sd = 6)
```

Checking means and sd's

```{r checking_descriptives, message=FALSE}
df_colonists %>% 
  filter(role == "scientist") %>% 
  summarize(
    mean_ps_scientist = mean(problem_solving),
    sd_ps_scientist = sd(problem_solving)
    )
df_colonists %>% 
  filter(role == "engineer") %>% 
  summarize(
    mean_ps_engineer = mean(problem_solving),
    sd_ps_engineer = sd(problem_solving)
    )
df_colonists %>% 
  filter(role == "medic") %>% 
  summarize(
    mean_ps_medic = mean(problem_solving),
    sd_ps_medic = sd(problem_solving)
    )
```

This seems to have worked (I tried it with a few different seeds and got the same rank ordering), though with the relatively small n the means and sd's are a bit off. That's to be expected. 

One question: In the code chunk "rnorm(sum(df_colonists$role == "engineer"), mean = 76, sd = 6)", why is the term "sum" there? I'm not seeing what is being summed.




> 3.2. Simulate Resilience and Agreeableness
Parameters for Simulation:
    n = 100: We are simulating attributes for 100 colonists.
    mu = c( 50, 50): Represents the average scores for Resilience and Agreeableness.
    Sigma = matrix(c(100, 50, 50, 100), ncol = 2): The covariance matrix, where diagonals represent variances and the off-diagonals represent the covariance between the traits.



```{r setting_original_seed, message=FALSE}
set.seed (123)
```


```{r simulating-data_with_given_correlations, message=FALSE}
library(MASS)

# Define mean and covariance matrix
mean_traits <- c(50, 50)
cov_matrix <- matrix(c(100, 50, 
                       50, 100), ncol = 2)

# Generate correlated data

traits_data <- mvrnorm(n = 100, mu = mean_traits, Sigma = cov_matrix, empirical = FALSE)

colnames(traits_data) <- c("resilience", "agreeableness")

df_colonists <- cbind(df_colonists, traits_data)
```


> But, now, we need to simulate more attributes for our colonists that don’t all have the same summary statistics. We need to simulate the big five personality traits: openness, conscientiousness, extraversion, agreeableness, and neuroticism. These traits are essential for understanding how colonists will interact with each other and cope with the challenges of living on Mars. Your task to simulate these traits using the mvrnorm function, using parameters that would make sense for these traits, and examine how closely your colonists match up with your population parameters.
   3.3. Simulate Big Five Traits using the Correlation Matrix provided and examine how closely your colonists match up with your parameters.
   Population Parameters for Big Five Traits I’ve provided a correlation matrix from Park et al. (2020)6 7 that you can use to get you started on simulating these traits.

I am choosing the following inputs (same order that you provided the correlation matrix).
means: 75, 80, 85, 85, 75
(My reasoning is that colonists need to be high in all of these. Perhaps extraversion and openness are the least important, but it's close.)
SDs:  5, 6, 4, 3, 7  (least variability in conscientiousness, most in openness)
VARs: 25, 36, 16, 9, 49

Note the hardest part here was converting the correlation matrix to a covariance matrix. I started doing that in excel, but then decided to get chatgpt's help with some R code for doing that.

```{r getting_inputs_for_big5, message=FALSE}

# Define means
mean_traits_big5 <- c(75, 80, 85, 85, 75)
# Define correlation matrix
corrmatrix_big5 <- matrix(c(
  1.0000, 0.2599, 0.1972, 0.1860, 0.2949,
  0.2599, 1.0000, 0.1576, 0.2306, 0.0720,
  0.1972, 0.1576, 1.0000, 0.2866, 0.1951,
  0.1860, 0.2306, 0.2866, 1.0000, 0.1574,
  0.2949, 0.0720, 0.1951, 0.1574, 1.0000
  ), nrow=5, ncol=5, byrow=TRUE,
  dimnames=list(c("EXT", "EmotSt", "AGR", "COOP", "OPEN"), 
        c("EXT", "EmotST", "AGR", "COOP", "OPEN")))
# Define Standard Deviations
sd_traits_big5 <- c(5, 6, 4, 3, 7)
# Define Covariance matrix
covmatrix_big5 <- corrmatrix_big5
  for (i in 1:nrow(corrmatrix_big5)) {
  for (j in 1:ncol(corrmatrix_big5)) {
    covmatrix_big5[i, j] <- corrmatrix_big5[i, j] * sd_traits_big5[i] * sd_traits_big5[j]
  }
  }

```

```{r simulating-big5data, message=FALSE}

# Generate correlated data

traits_data_big5 <- mvrnorm(n = 100, mu = mean_traits_big5, Sigma = covmatrix_big5, empirical = FALSE)

df_colonists <- cbind(df_colonists, traits_data_big5)
```

This seems to have worked. Yea!!  The covariance matrix looks reasonable to me, as does the data set. Nonetheless, I'll go ahead and check this to make sure that it produced data that at least approximates the actual parameters.

```{r testing_big5_means_sds, message=FALSE}
df_colonists %>% 
  summarize(
    mean_ext = mean(EXT),
    mean_emotst = mean(EmotSt),
    mean_agr = mean(AGR),
    mean_coop = mean(COOP),
    mean_open = mean(OPEN),
    sd_ext = sd(EXT),
    sd_emotst = sd(EmotSt),
    sd_agr = sd(AGR),
    sd_coop = sd(COOP),
    sd_open = sd(OPEN)
    )
```

These look pretty close!  Now checking correlations.

```{r testing_big5_corrs, message=FALSE}
df_colonists %>% 
  summarize(
    ext_emotst = cor(EXT, EmotSt),
    ext_agr = cor(EXT, AGR),    
    ext_coop = cor(EXT, COOP),
    ext_open = cor(EXT, OPEN),
    emotst_agr = cor(AGR, EmotSt),
    emotst_coop = cor(COOP, EmotSt),
    emotst_open = cor(OPEN, EmotSt),
    agr_coop = cor(AGR, COOP),
    agr_open = cor(AGR, OPEN),
    coop_open = cor(COOP, OPEN)
            )
```

These are obviously far from exact, but seem pretty close, in that the smaller correlations in the population are typically the smallest correlations here too.

> Great! We have successfully simulated a dataset of 100 colonists with interdependent skills. The summary statistics show that the mean and standard deviation of the simulated data match the population parameters. The correlation matrix also aligns (fairly well) with the specified values. But did we just get lucky? Let’s run the simulation multiple times to ensure the results are consistent across different scenarios. 

Okay, I'm going to rerun this now using a different seed.

```{r redoing_with_seed_of_2, message=FALSE}

set.seed(2)

# Generate correlated data

traits_data_big5_2 <- mvrnorm(n = 100, mu = mean_traits_big5, Sigma = covmatrix_big5, empirical = FALSE)

# Need to change the variable names before combining

traits_data_big5_2 <- as.data.frame(traits_data_big5_2)

traits_data_big5_2 <- traits_data_big5_2 %>%
  rename(
    EXT_2 = EXT,
    EmotSt_2 = EmotSt,
    AGR_2 = AGR,
    COOP_2 = COOP,
    OPEN_2 = OPEN
  )

# Now combining

df_colonists <- cbind(df_colonists, traits_data_big5_2)

df_colonists %>% 
  summarize(
    mean_ext = mean(EXT_2),
    mean_emotst = mean(EmotSt_2),
    mean_agr = mean(AGR_2),
    mean_coop = mean(COOP_2),
    mean_open = mean(OPEN_2),
    sd_ext = sd(EXT_2),
    sd_emotst = sd(EmotSt_2),
    sd_agr = sd(AGR_2),
    sd_coop = sd(COOP_2),
    sd_open = sd(OPEN_2)
    )
df_colonists %>% 
  summarize(
    ext_emotst = cor(EXT_2, EmotSt_2),
    ext_agr = cor(EXT_2, AGR_2),    
    ext_coop = cor(EXT_2, COOP_2),
    ext_open = cor(EXT_2, OPEN_2),
    emotst_agr = cor(AGR_2, EmotSt_2),
    emotst_coop = cor(COOP_2, EmotSt_2),
    emotst_open = cor(OPEN_2, EmotSt_2),
    agr_coop = cor(AGR_2, COOP_2),
    agr_open = cor(AGR_2, OPEN_2),
    coop_open = cor(COOP_2, OPEN_2)
            )
```

Again, this seems to have worked pretty well. 


Note I just checked how you approached this question. It looks similar, though you have a different command for computing the covariance matrix. Yours looks to be more efficient, but I don't understand it (there's a weird t in the command). Perhaps we can discuss this on sunday.


```{r original_seed, message=FALSE}

set.seed(123)

```


> Exercise 4: Preparing for the Unexpected
    Generate the big five for 100 colonies of 100 colonists, repeating this process multiple times to how much our colony might look if we settled on 100 different planets. We’ve already made the colonists for one planet, so we’ll just need to replicate that process 99 more times. There are two major approaches to take here. You can either use replicate, which is easier, or a for loop, which is more flexible.

> 4.1. Generate 100 colonies and extract the mean and standard deviation for extraversion.

```{r removing-previous-data, message=FALSE}

# Taking out previous values for the Big 5

df_colonists <- df_colonists %>% select(-EXT_2, -EmotSt_2, -AGR_2, -COOP_2, -OPEN_2)

```

It turns out I did not need to do the above. I hadn't realized quite how things would work so wanted to take out the data with a different seed. I'm leaving this here, however, because it's a nice example of how to remove variables from a data frame and I might want to use this later.



```{r replicating_100_colonies, message=FALSE}

colonies_data <- replicate(100, {
  mvrnorm(n = 100, mu = mean_traits_big5, Sigma = covmatrix_big5, empirical = FALSE)
}, simplify = FALSE)

# This is in a list format, which is hard to see, so I converted to a data frame.

combined_data <- do.call(rbind, lapply(seq_along(colonies_data), function(i) {
  cbind(colony = i, colonies_data[[i]])
}))

combined_data <- as.data.frame(combined_data)

```

Now computing the desired statistics 

```{r extraversion_means_sds, message=FALSE}

summary_ext <- combined_data %>%
  group_by(colony) %>%
  summarize(
    mean_ext_per_colony = mean(EXT, na.rm = TRUE),
    sd_ext_per_colony = sd(EXT, na.rm = TRUE)
  )

summary_ext %>% 
  summarize(
    mean_ext = mean(mean_ext_per_colony),
    sd_ext = mean(sd_ext_per_colony)
    )

```

These are pretty much perfect.

This exercise took me quite some time, because I wanted to do this without looking at your hints. I couldn't figure out how to do this without Chat's help, but the way it set up the code initially didn't make sense to me. So I iterated this multiple times (good practice for both classes...) with Chat, and finally found the step by step approach above that makes sense to me, that I can visualize, and that I think I can adapt for future uses. (The one partial exception is the conversion of the list to the data frame -- I see mostly what's happening here, but not precisely how the commands work.)

> Although you can get the summary statistic for each variable, let’s focus on the mean and standard deviation for extraversion as well as its correlation with openness. Please plot the distribution of the correlation coefficients to see how consistent the relationship between extraversion and openness is across different planets. Consider how this distribution might differ across colonists. How large of a sample size would you need to get a stable estimate of the correlation between extraversion and openness?

```{r correlations_between_extraversion_and_openness, message=FALSE}

summary_ext_open_corr <- combined_data %>%
  group_by(colony) %>%
  summarize(
    ext_open_corr = cor(EXT, OPEN)
  )

summary_ext_open_corr %>% 
  summarize(
    mean_ext_open_corr = mean(ext_open_corr)
    )

```

The correlation parameter is .2949, so on average (across the 100 colonies) we are doing an excellent job estimating the true population correlation.

Now I need to plot the distribution of scores. I remember from an earlier lab having trouble creating a grouped histogram. I found what I did there and adapated it for this,  though it seems like there should be an easier way.


```{r plotting_distribution, message = FALSE}
summary_ext_open_corr <- summary_ext_open_corr %>%
  mutate(corr_grouped = case_when(
    ext_open_corr > 0 & ext_open_corr <= .05 ~ "0-.05",
    ext_open_corr > .05 & ext_open_corr <= .10 ~ ".05-.10",
    ext_open_corr > .10 & ext_open_corr <= .15 ~ ".10-.15",
    ext_open_corr > .15 & ext_open_corr <= .20 ~ ".15-.20",
    ext_open_corr > .20 & ext_open_corr <= .25 ~ ".20-.25",
    ext_open_corr > .25 & ext_open_corr <= .30 ~ ".25-.30",
    ext_open_corr > .30 & ext_open_corr <= .35 ~ ".30-.35",
    ext_open_corr > .35 & ext_open_corr <= .40 ~ ".35-.40",
    ext_open_corr > .40 & ext_open_corr <= .45 ~ ".40-.45",
    ext_open_corr > .45 & ext_open_corr <= .50 ~ ".45-.50"
  ))
summary_ext_open_corr_with_n <- summary_ext_open_corr %>%
  count(corr_grouped) %>%
  complete(corr_grouped, fill = list(n = 0))
ggplot(summary_ext_open_corr_with_n, aes(x = corr_grouped, y=n)) + 
  geom_bar(stat="identity") +
  labs(
    title = "Number of correlations in each range",
    x = "Correlation", y = "Count"
  )
```

You can tell from the graph that the average correlation is pretty much right on, though there is quite a bit of variability around that. 

You asked how large an n we'd need to get a stable estimate of the correlation. Clearly n = 100 is not remotely close. 1,000 is pretty good, though it depends on just how precise one wants to be. For most practical purposes n = 1,000 seems like more than enough, but to report in a paper I'd want more than that (at least 10,000, perhaps more).
  
  > 4.2. Plot the distributions of those statistics from your new empire of colonies, and include an annotation in the plot to show the population parameters.

I think this question is asking me to redo the above but with the population correlation listed on it, which seems useful. Here we go:

```{r plotting_distribution_with_population_correlation, message = FALSE}
summary_ext_open_corr <- summary_ext_open_corr %>%
  mutate(corr_grouped = case_when(
    ext_open_corr > 0 & ext_open_corr <= .05 ~ "0-.05",
    ext_open_corr > .05 & ext_open_corr <= .10 ~ ".05-.10",
    ext_open_corr > .10 & ext_open_corr <= .15 ~ ".10-.15",
    ext_open_corr > .15 & ext_open_corr <= .20 ~ ".15-.20",
    ext_open_corr > .20 & ext_open_corr <= .25 ~ ".20-.25",
    ext_open_corr > .25 & ext_open_corr <= .30 ~ ".25-.30",
    ext_open_corr > .30 & ext_open_corr <= .35 ~ ".30-.35",
    ext_open_corr > .35 & ext_open_corr <= .40 ~ ".35-.40",
    ext_open_corr > .40 & ext_open_corr <= .45 ~ ".40-.45",
    ext_open_corr > .45 & ext_open_corr <= .50 ~ ".45-.50"
  ))
summary_ext_open_corr_with_n <- summary_ext_open_corr %>%
  count(corr_grouped) %>%
  complete(corr_grouped, fill = list(n = 0))
ggplot(summary_ext_open_corr_with_n, aes(x = corr_grouped, y = n)) + 
  geom_bar(stat = "identity") +
  geom_vline(xintercept = 5.45, linetype = "dashed", color = "red", linewidth = 1) +  # Adjust xintercept to match position
  annotate("text", x = 5.45, y = max(summary_ext_open_corr_with_n$n), label = "Population Correlation = 0.2949", color = "red", vjust = -0.5) +
  labs(
    title = "Number of correlations in each range",
    x = "Correlation", y = "Count"
  )
```
